# ğŸ—ï¸ DataFoundry â€“ Robust Data Pipeline for Crypto Analytics

**DataFoundry** is a modern data pipeline designed to ingest, validate, transform and test historical Bitcoin market data. It combines high-performance tools like Polars, DuckDB, dbt, and Prefect to ensure data quality, reproducibility and pipeline observability.

---

## ğŸš€ Project Goal

The goal of this pipeline is to prepare reliable, tested and ready-to-analyze Bitcoin data for downstream reporting or analytics platforms. It demonstrates modern data engineering practices such as:

- Schema and value validation (Polars assertions)
- Model-driven transformations (dbt)
- Declarative testing (dbt tests)
- Flow orchestration (Prefect)
- Reproducible environment setup

---

## âš™ï¸ Stack

- [Polars](https://pola-rs.github.io/polars/) â€“ Data ingestion and validation
- [DuckDB](https://duckdb.org/) â€“ Lightweight OLAP database for local development
- [dbt-duckdb](https://docs.getdbt.com/docs/core/connect-data-platform/duckdb) â€“ Data transformation + testing layer
- [Prefect](https://docs.prefect.io/) â€“ Workflow orchestration
- [VSCode DevContainer] â€“ Reproducible environment

---

## ğŸ“ˆ Flow Diagram (ASCII)

```
[CoinGecko API]
      â”‚
      â–¼
[fetch_btc_data (Polars)]
      â”‚
      â–¼
[validate_with_polars] -> [failed]
      â”‚
      â–¼
[save_to_csv â†’ data/input.csv]
      â”‚
      â–¼
[load_to_duckdb â†’ foundry.duckdb/raw_data]
      â”‚
      â–¼
[dbt run â†’ btc_daily_stats]
      â”‚
      â–¼
[dbt test] -> [failed]
```

## ğŸ“ Project Structure

```
data-foundry/
â”œâ”€â”€ data/                        # CSVs and local outputs
â”œâ”€â”€ dbt_project/                # dbt config and models
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ btc_daily_stats.sql
â”‚   â”‚   â””â”€â”€ btc_daily_stats.yml
â”œâ”€â”€ prefect_flows/             # Prefect + Polars pipeline code
â”‚   â””â”€â”€ pipeline.py
â”‚   â””â”€â”€ validation_tasks.py
â”œâ”€â”€ foundry.duckdb             # Local DuckDB file
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## â–¶ï¸ Running the Pipeline

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run the pipeline

```bash
python prefect_flows/pipeline.py
```

This will:

- Fetch daily Bitcoin market data from CoinGecko
- Validate the DataFrame using Polars assertions
- Save raw CSV locally (`data/input.csv`)
- Load the data into a DuckDB table (`raw_data`)
- Run dbt transformations â†’ `btc_daily_stats`
- Run dbt tests to assert schema and value integrity

---

## ğŸ§ª Validation Logic

### âœ… Polars (in-Python)

- `price` > 10,000
- `market_cap` > 0
- `volume` not null
- `date` unique

### âœ… dbt Tests

Defined in `btc_daily_stats.yml`:

- not_null + range test for `price_usd`
- not_null for `market_cap_billion`, `volume_million`
- unique for `date`

---

## ğŸ‘ï¸ Flow UI (Optional)

If using Prefect locally:

```bash
prefect server start
```

Then visit: `http://localhost:4200`

---

## ğŸ“Œ Future Enhancements

- Add `pytest`-based unit tests for each task
- Add Great Expectations (optional)
- Generate HTML docs with `dbt docs generate`
- Add CI pipeline (GitHub Actions)
- Integrate visualization layer (Metabase, Streamlit, etc.)

---

Built by **Agustin** â€“ aiming to demonstrate senior-level engineering practices through clean design, modular structure and data quality focus. âœ¨
